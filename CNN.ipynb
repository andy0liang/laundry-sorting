{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85922cd2-5ed4-44b9-9f3a-fde0eff217ea",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89cc8e-2217-441a-8aa0-4d35ad117795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from simclr import SimCLR\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae96513-428f-4655-9504-d42f045b63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "datapath = 'clothing-dataset/'\n",
    "data = []\n",
    "with open(os.path.join(datapath, 'images.csv')) as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=',')\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2fcdf-b963-4bdc-8ad5-ea75ccbe2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584fafb3-d228-4f04-bbe1-c10b8f9b2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[0]\n",
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a017d-2769-4f46-8870-4f03fb09f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f6f5a-1295-4aa0-b700-195d18f916b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d49f8-1f00-4138-8c1e-1cfc3289ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = dict()\n",
    "for row in data:\n",
    "    if row[2] not in label_counts:\n",
    "        label_counts[row[2]] = 0\n",
    "    label_counts[row[2]] += 1\n",
    "    \n",
    "valid_labels = dict()\n",
    "for k, v in label_counts.items():\n",
    "    if v >= 100 and k not in ['Not sure', 'Others', 'Skip']:\n",
    "        valid_labels[k] = v\n",
    "\n",
    "filename_to_label = dict()\n",
    "cleaned_data = []\n",
    "valid_filenames = []\n",
    "for i in range(len(data)):\n",
    "    if data[i][2] in valid_labels.keys():\n",
    "        cleaned_data.append(data[i])\n",
    "        valid_filenames.append(data[i][0])\n",
    "        filename_to_label[data[i][0]] = data[i][2]\n",
    "\n",
    "\n",
    "\n",
    "num_examples = len(cleaned_data)\n",
    "print(num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e4a5d-811c-43f2-bf02-8a253c58ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "imagespath = 'clothing-dataset/images'\n",
    "images = [f for f in os.listdir(imagespath) if os.path.isfile(os.path.join(imagespath, f))]\n",
    "\n",
    "num_images_moved = 0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(images)):\n",
    "    filename = images[i].split(\".\")[0]\n",
    "    if filename not in valid_filenames:\n",
    "        continue\n",
    "        \n",
    "    if num_images_moved < num_train + num_validate:\n",
    "        folder_name = 'train'\n",
    "    else:\n",
    "        folder_name = 'test'\n",
    "    \n",
    "    new_folder = os.path.join(imagespath, folder_name)\n",
    "    \n",
    "    classname = filename_to_label[filename]\n",
    "    finalfoldername = os.path.join(new_folder, classname)\n",
    "    \n",
    "    if not os.path.exists(finalfoldername):\n",
    "        os.makedirs(finalfoldername)\n",
    "    \n",
    "    old_image_path = os.path.join(imagespath, images[i])\n",
    "    new_image_path = os.path.join(finalfoldername, images[i])\n",
    "    shutil.move(old_image_path, new_image_path)\n",
    "    num_images_moved += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60d542-006b-459c-b28b-c720b6229169",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 3600\n",
    "num_validate = 900\n",
    "num_test = 461\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "train_dir = os.path.join(os.getcwd(), os.path.join(imagespath, 'train'))\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = len(valid_labels)\n",
    "labels = [\"Blazer\", 'Dress', 'Hat', 'Hoodie', 'Longsleeve', 'Outwear', 'Pants', 'Polo', 'Shirt', 'Shoes', 'Shorts', 'Skirt', 'T-Shirt', 'Undershirt']\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a18531a-c23d-4bf8-b758-e3d3db07164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = random.randint(1, 10000)\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c54a86-f998-41a9-b223-441019acf585",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmao = train_ds.take(1)\n",
    "fig, ax = plt.subplots(3, 3)\n",
    "for x, y in tfds.as_numpy(lmao):\n",
    "    for i in range(batch_size):\n",
    "        if i < 9:\n",
    "            ax[i // 3, i % 3].imshow(x[i] / 255)\n",
    "            ax[i // 3, i % 3].set_title(labels[y[i]])\n",
    "            ax[i // 3, i % 3].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb8b9db-28f4-409a-8fb7-22c4c9e487a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"), \n",
    "                                         tf.keras.layers.RandomRotation(0.2),])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b0137-63fa-44de-a9cf-c6bdb1ce4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    data_augmentation, \n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800daa2f-8f8f-42e5-8aeb-2eac309e8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75435d4-93ed-4601-96f7-55cfd2e96bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7e8b5-502b-4f44-a0b8-f52592dacfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = tf.keras.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "predictor.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "  metrics=['accuracy'])\n",
    "accuracy = predictor.evaluate(test_ds)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757c074f-7573-4d86-bc6d-692b58a92ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = os.path.join(os.getcwd(), os.path.join(imagespath, 'test'))\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_dir,\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46668c8f-5c20-46a0-860e-c6aeb1a85c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder():\n",
    "    transfer_model = tf.keras.Sequential()\n",
    "    resnet50 = tf.keras.applications.ResNet50V2(include_top=False,\n",
    "                       input_shape=(img_height,img_width,3),\n",
    "                       pooling='avg',classes=num_classes,\n",
    "                       weights='imagenet')\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    transfer_model.add(data_augmentation)\n",
    "\n",
    "    transfer_model.add(resnet50)\n",
    "    return transfer_model\n",
    "\n",
    "\n",
    "def create_classifier(encoder, trainable=True):\n",
    "    for layer in encoder.layers:\n",
    "        layer.trainable=trainable\n",
    "    encoder.add(Dropout(0.1))\n",
    "    encoder.add(Flatten())\n",
    "    encoder.add(Dense(128, activation='relu'))\n",
    "    encoder.add(Dropout(0.1))\n",
    "    encoder.add(Dense(num_classes))\n",
    "    return encoder\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857558c-585a-451a-9de5-2b84c55a3348",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = tf.keras.Sequential()\n",
    "resnet50 = tf.keras.applications.ResNet50V2(include_top=False,\n",
    "                    input_shape=(img_height,img_width,3),\n",
    "                    pooling='avg',classes=num_classes,\n",
    "                    weights='imagenet')\n",
    "\n",
    "\n",
    "transfer_model.add(data_augmentation)\n",
    "\n",
    "for layer in resnet50.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "transfer_model.add(resnet50)\n",
    "transfer_model.add(Flatten())\n",
    "transfer_model.add(tf.keras.layers.BatchNormalization())\n",
    "transfer_model.add(Dense(128, activation='relu'))\n",
    "transfer_model.add(Dropout(0.1))\n",
    "transfer_model.add(tf.keras.layers.BatchNormalization())\n",
    "transfer_model.add(Dense(num_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8ec11-f1bb-4d95-abdd-b7d29c454b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transfer_model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "history = transfer_model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c654a-b234-48e0-b3d6-5a7c1d8d82dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = classifier.evaluate(test_ds)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060c81e-ccb8-424a-b234-edf823137d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = transfer_model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6720e-824b-4ed8-9e51-ef348c06f4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"Blazer\", 'Dress', 'Hat', 'Hoodie', 'Longsleeve', 'Outwear', 'Pants', 'Polo', 'Shirt', 'Shoes', 'Shorts', 'Skirt', 'T-Shirt', 'Undershirt']\n",
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad393f7-782c-4c12-b788-28304841286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_labels = np.argmax(predictions, axis=1)\n",
    "pred_classes = np.array(labels)[pred_labels]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a1de4e-ff9b-45ee-81d5-e15e009433a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7f7698-c95a-4556-b082-23c7458debbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164065db-11a8-4692-8b13-5e19e28fbf57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5060b7e-f403-46ce-9340-6c6eb35b7abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature=1, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        # Normalize feature vectors\n",
    "        feature_vectors_normalized = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        # Compute logits\n",
    "        logits = tf.divide(\n",
    "            tf.matmul(\n",
    "                feature_vectors_normalized, tf.transpose(feature_vectors_normalized)\n",
    "            ),\n",
    "            self.temperature,\n",
    "        )\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), logits)\n",
    "\n",
    "\n",
    "def add_projection_head(encoder):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(features)\n",
    "    model = keras.Model(\n",
    "        inputs=inputs, outputs=outputs, name=\"cifar-encoder_with_projection-head\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a818ea-2b40-4e58-93af-3f71d8ed1fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "temperature = 0.05\n",
    "projection_units = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ec9530-73cb-42ea-8362-3f6c681dcb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = create_encoder()\n",
    "\n",
    "encoder_with_projection_head = add_projection_head(encoder)\n",
    "encoder_with_projection_head.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(temperature),\n",
    ")\n",
    "\n",
    "encoder_with_projection_head.summary()\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "history = encoder_with_projection_head.fit(\n",
    "    train_ds, validation_data=val_ds, batch_size=batch_size, epochs=epochs\n",
    ")\n",
    "\n",
    "classifier = create_classifier(encoder, trainable=False)\n",
    "\n",
    "classifier.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "history = classifier.fit(train_ds, validation_data=val_ds, batch_size=batch_size, epochs=epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac43a4e-1e5a-4a4e-87e2-a3eceec81a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = tf.keras.Sequential([classifier, tf.keras.layers.Softmax()])\n",
    "\n",
    "predictor.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "  metrics=['accuracy'])\n",
    "\n",
    "accuracy = classifier.evaluate(test_ds)[1]\n",
    "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d80ac5-53c5-4add-8bcf-f990db00c5db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd021f8-e64d-4d26-8d11-c0b5ad7338d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a2c4c-1a58-4e8c-8746-95bbb890f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Algorithm = SimCLR\n",
    "\n",
    "width = 128\n",
    "num_epochs = 10\n",
    "\n",
    "hyperparams = {SimCLR: {\"temperature\": 0.1}}\n",
    "\n",
    "# architecture\n",
    "model = Algorithm(\n",
    "    contrastive_augmenter=keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(img_width, img_height, 3)),\n",
    "            preprocessing.Rescaling(1 / 255),\n",
    "            preprocessing.RandomFlip(\"horizontal\"),\n",
    "            \n",
    "        ],\n",
    "        name=\"contrastive_augmenter\",\n",
    "    ),\n",
    "    classification_augmenter=keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(img_width, img_height, 3)),\n",
    "            preprocessing.Rescaling(1 / 255),\n",
    "            preprocessing.RandomFlip(\"horizontal\"),\n",
    "            \n",
    "        ],\n",
    "        name=\"classification_augmenter\",\n",
    "    ),\n",
    "    encoder=keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(img_width, img_height, 3)),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Conv2D(width, kernel_size=3, strides=2, activation=\"relu\"),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(width, activation=\"relu\"),\n",
    "        ],\n",
    "        name=\"encoder\",\n",
    "    ),\n",
    "    projection_head=keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(width,)),\n",
    "            layers.Dense(width, activation=\"relu\"),\n",
    "            layers.Dense(width),\n",
    "        ],\n",
    "        name=\"projection_head\",\n",
    "    ),\n",
    "    linear_probe=keras.Sequential(\n",
    "        [\n",
    "            layers.Input(shape=(width,)),\n",
    "            layers.Dense(num_classes),\n",
    "        ],\n",
    "        name=\"linear_probe\",\n",
    "    ),\n",
    "    **hyperparams[Algorithm],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6288bb-ff65-41cc-9b30-4999ae7eadd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    contrastive_optimizer=keras.optimizers.Adam(),\n",
    "    probe_optimizer=keras.optimizers.Adam(),\n",
    ")\n",
    "\n",
    "# run training\n",
    "\n",
    "np_train_ds = tfds.as_numpy(train_ds)\n",
    "np_val_ds = tfds.as_numpy(val_ds)\n",
    "\n",
    "\n",
    "    \n",
    "history = model.fit(np_train_ds, epochs=num_epochs, validation_data=np_val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce9e9c3-f3a9-449c-9c59-54320336a668",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
