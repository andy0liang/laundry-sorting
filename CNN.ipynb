{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85922cd2-5ed4-44b9-9f3a-fde0eff217ea",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b89cc8e-2217-441a-8aa0-4d35ad117795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dae96513-428f-4655-9504-d42f045b63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "datapath = 'clothing-dataset/'\n",
    "data = []\n",
    "with open(os.path.join(datapath, 'images.csv')) as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=',')\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "65c2fcdf-b963-4bdc-8ad5-ea75ccbe2e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['image', 'sender_id', 'label', 'kids'], ['4285fab0-751a-4b74-8e9b-43af05deee22', '124', 'Not sure', 'False'], ['ea7b6656-3f84-4eb3-9099-23e623fc1018', '148', 'T-Shirt', 'False'], ['00627a3f-0477-401c-95eb-92642cbe078d', '94', 'Not sure', 'False'], ['ea2ffd4d-9b25-4ca8-9dc2-bd27f1cc59fa', '43', 'T-Shirt', 'False'], ['3b86d877-2b9e-4c8b-a6a2-1d87513309d0', '189', 'Shoes', 'False'], ['5d3a1404-697f-479f-9090-c1ecd0413d27', '138', 'Shorts', 'False'], ['b0c03127-9dfb-4573-8934-1958396937bf', '138', 'Shirt', 'False'], ['4c8f245e-a039-46fd-a6b9-1bb51e83fc05', '226', 'T-Shirt', 'False'], ['c995c900-693d-4dd6-8995-43f3051ec488', '337', 'Pants', 'False']]\n"
     ]
    }
   ],
   "source": [
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "584fafb3-d228-4f04-bbe1-c10b8f9b2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[0]\n",
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "109a017d-2769-4f46-8870-4f03fb09f552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['image', 'sender_id', 'label', 'kids']\n",
      "[['4285fab0-751a-4b74-8e9b-43af05deee22', '124', 'Not sure', 'False'], ['ea7b6656-3f84-4eb3-9099-23e623fc1018', '148', 'T-Shirt', 'False'], ['00627a3f-0477-401c-95eb-92642cbe078d', '94', 'Not sure', 'False'], ['ea2ffd4d-9b25-4ca8-9dc2-bd27f1cc59fa', '43', 'T-Shirt', 'False'], ['3b86d877-2b9e-4c8b-a6a2-1d87513309d0', '189', 'Shoes', 'False'], ['5d3a1404-697f-479f-9090-c1ecd0413d27', '138', 'Shorts', 'False'], ['b0c03127-9dfb-4573-8934-1958396937bf', '138', 'Shirt', 'False'], ['4c8f245e-a039-46fd-a6b9-1bb51e83fc05', '226', 'T-Shirt', 'False'], ['c995c900-693d-4dd6-8995-43f3051ec488', '337', 'Pants', 'False'], ['bf78527f-0734-44fd-a968-f6c32c12d61e', '175', 'Shoes', 'False']]\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb1f6f5a-1295-4aa0-b700-195d18f916b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5403\n"
     ]
    }
   ],
   "source": [
    "num_examples = len(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a1d49f8-1f00-4138-8c1e-1cfc3289ce72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4961\n"
     ]
    }
   ],
   "source": [
    "label_counts = dict()\n",
    "for row in data:\n",
    "    if row[2] not in label_counts:\n",
    "        label_counts[row[2]] = 0\n",
    "    label_counts[row[2]] += 1\n",
    "    \n",
    "valid_labels = dict()\n",
    "for k, v in label_counts.items():\n",
    "    if v >= 100 and k not in ['Not sure', 'Others', 'Skip']:\n",
    "        valid_labels[k] = v\n",
    "\n",
    "filename_to_label = dict()\n",
    "cleaned_data = []\n",
    "valid_filenames = []\n",
    "for i in range(len(data)):\n",
    "    if data[i][2] in valid_labels.keys():\n",
    "        cleaned_data.append(data[i])\n",
    "        valid_filenames.append(data[i][0])\n",
    "        filename_to_label[data[i][0]] = data[i][2]\n",
    "\n",
    "\n",
    "\n",
    "num_examples = len(cleaned_data)\n",
    "print(num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e43e4a5d-811c-43f2-bf02-8a253c58ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "imagespath = 'clothing-dataset/images'\n",
    "images = [f for f in os.listdir(imagespath) if os.path.isfile(os.path.join(imagespath, f))]\n",
    "\n",
    "num_images_moved = 0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(images)):\n",
    "    filename = images[i].split(\".\")[0]\n",
    "    if filename not in valid_filenames:\n",
    "        continue\n",
    "        \n",
    "    if num_images_moved < num_train + num_validate:\n",
    "        folder_name = 'train'\n",
    "    else:\n",
    "        folder_name = 'test'\n",
    "    \n",
    "    new_folder = os.path.join(imagespath, folder_name)\n",
    "    \n",
    "    classname = filename_to_label[filename]\n",
    "    finalfoldername = os.path.join(new_folder, classname)\n",
    "    \n",
    "    if not os.path.exists(finalfoldername):\n",
    "        os.makedirs(finalfoldername)\n",
    "    \n",
    "    old_image_path = os.path.join(imagespath, images[i])\n",
    "    new_image_path = os.path.join(finalfoldername, images[i])\n",
    "    shutil.move(old_image_path, new_image_path)\n",
    "    num_images_moved += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e60d542-006b-459c-b28b-c720b6229169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "num_train = 3600\n",
    "num_validate = 900\n",
    "num_test = 461\n",
    "epochs = 15\n",
    "batch_size = 16\n",
    "train_dir = os.path.join(os.getcwd(), os.path.join(imagespath, 'train'))\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "num_classes = len(valid_labels)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a18531a-c23d-4bf8-b758-e3d3db07164a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4500 files belonging to 14 classes.\n",
      "Using 3600 files for training.\n",
      "Found 4500 files belonging to 14 classes.\n",
      "Using 900 files for validation.\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "seed = random.randint(1, 10000)\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "958b0137-63fa-44de-a9cf-c6bdb1ce4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "800daa2f-8f8f-42e5-8aeb-2eac309e8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e75435d4-93ed-4601-96f7-55cfd2e96bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "225/225 [==============================] - 42s 185ms/step - loss: 2.6665 - accuracy: 0.2831 - val_loss: 1.9309 - val_accuracy: 0.4344\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 42s 185ms/step - loss: 1.6110 - accuracy: 0.5178 - val_loss: 1.8048 - val_accuracy: 0.4822\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 42s 185ms/step - loss: 0.8557 - accuracy: 0.7503 - val_loss: 2.2285 - val_accuracy: 0.4856\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 42s 186ms/step - loss: 0.3427 - accuracy: 0.9025 - val_loss: 2.6812 - val_accuracy: 0.4722\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 42s 186ms/step - loss: 0.1447 - accuracy: 0.9608 - val_loss: 3.3321 - val_accuracy: 0.5022\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 42s 188ms/step - loss: 0.0843 - accuracy: 0.9833 - val_loss: 3.4140 - val_accuracy: 0.5133\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 42s 186ms/step - loss: 0.0701 - accuracy: 0.9886 - val_loss: 3.8468 - val_accuracy: 0.4600\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 42s 188ms/step - loss: 0.1079 - accuracy: 0.9806 - val_loss: 3.9581 - val_accuracy: 0.4622\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 43s 190ms/step - loss: 0.1273 - accuracy: 0.9683 - val_loss: 4.4612 - val_accuracy: 0.4500\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 42s 186ms/step - loss: 0.0562 - accuracy: 0.9867 - val_loss: 4.9422 - val_accuracy: 0.4622\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 42s 188ms/step - loss: 0.0490 - accuracy: 0.9900 - val_loss: 5.4884 - val_accuracy: 0.4678\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 42s 187ms/step - loss: 0.0876 - accuracy: 0.9811 - val_loss: 4.6052 - val_accuracy: 0.4456\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 42s 186ms/step - loss: 0.0480 - accuracy: 0.9881 - val_loss: 5.1447 - val_accuracy: 0.4678\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 42s 188ms/step - loss: 0.0281 - accuracy: 0.9939 - val_loss: 6.5060 - val_accuracy: 0.4356\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 42s 186ms/step - loss: 0.0385 - accuracy: 0.9914 - val_loss: 5.4476 - val_accuracy: 0.4556\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "757c074f-7573-4d86-bc6d-692b58a92ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 461 files belonging to 14 classes.\n"
     ]
    }
   ],
   "source": [
    "test_dir = os.path.join(os.getcwd(), os.path.join(imagespath, 'test'))\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_dir,\n",
    "  seed=123,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "46668c8f-5c20-46a0-860e-c6aeb1a85c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "225/225 [==============================] - 68s 298ms/step - loss: 1.3285 - accuracy: 0.5919 - val_loss: 0.9304 - val_accuracy: 0.6856\n",
      "Epoch 2/15\n",
      "225/225 [==============================] - 67s 297ms/step - loss: 0.6879 - accuracy: 0.7756 - val_loss: 0.8554 - val_accuracy: 0.7211\n",
      "Epoch 3/15\n",
      "225/225 [==============================] - 67s 298ms/step - loss: 0.4711 - accuracy: 0.8425 - val_loss: 0.8861 - val_accuracy: 0.7200\n",
      "Epoch 4/15\n",
      "225/225 [==============================] - 67s 297ms/step - loss: 0.3237 - accuracy: 0.8978 - val_loss: 0.9193 - val_accuracy: 0.7233\n",
      "Epoch 5/15\n",
      "225/225 [==============================] - 67s 297ms/step - loss: 0.2056 - accuracy: 0.9394 - val_loss: 0.8896 - val_accuracy: 0.7400\n",
      "Epoch 6/15\n",
      "225/225 [==============================] - 67s 297ms/step - loss: 0.1374 - accuracy: 0.9653 - val_loss: 0.9435 - val_accuracy: 0.7367\n",
      "Epoch 7/15\n",
      "225/225 [==============================] - 67s 298ms/step - loss: 0.0857 - accuracy: 0.9850 - val_loss: 1.0543 - val_accuracy: 0.7222\n",
      "Epoch 8/15\n",
      "225/225 [==============================] - 67s 299ms/step - loss: 0.0567 - accuracy: 0.9894 - val_loss: 1.0536 - val_accuracy: 0.7311\n",
      "Epoch 9/15\n",
      "225/225 [==============================] - 68s 302ms/step - loss: 0.0431 - accuracy: 0.9942 - val_loss: 1.1201 - val_accuracy: 0.7333\n",
      "Epoch 10/15\n",
      "225/225 [==============================] - 67s 296ms/step - loss: 0.0352 - accuracy: 0.9947 - val_loss: 1.2002 - val_accuracy: 0.7278\n",
      "Epoch 11/15\n",
      "225/225 [==============================] - 66s 295ms/step - loss: 0.0374 - accuracy: 0.9928 - val_loss: 1.1632 - val_accuracy: 0.7400\n",
      "Epoch 12/15\n",
      "225/225 [==============================] - 66s 294ms/step - loss: 0.0259 - accuracy: 0.9956 - val_loss: 1.2254 - val_accuracy: 0.7322\n",
      "Epoch 13/15\n",
      "225/225 [==============================] - 67s 296ms/step - loss: 0.1318 - accuracy: 0.9581 - val_loss: 1.5481 - val_accuracy: 0.6867\n",
      "Epoch 14/15\n",
      "225/225 [==============================] - 67s 297ms/step - loss: 0.1489 - accuracy: 0.9506 - val_loss: 1.4691 - val_accuracy: 0.7144\n",
      "Epoch 15/15\n",
      "225/225 [==============================] - 67s 297ms/step - loss: 0.0404 - accuracy: 0.9892 - val_loss: 1.5084 - val_accuracy: 0.7078\n"
     ]
    }
   ],
   "source": [
    "transfer_model = tf.keras.Sequential()\n",
    "resnet50 = tf.keras.applications.ResNet50(include_top=False,\n",
    "                   input_shape=(128,128,3),\n",
    "                   pooling='avg',classes=num_classes,\n",
    "                   weights='imagenet')\n",
    "\n",
    "for layer in resnet50.layers:\n",
    "    layer.trainable=False\n",
    "    \n",
    "transfer_model.add(resnet50)\n",
    "\n",
    "transfer_model.add(Flatten())\n",
    "transfer_model.add(Dense(128, activation='relu'))\n",
    "transfer_model.add(Dense(num_classes))\n",
    "\n",
    "transfer_model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "history = transfer_model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "053f9b9f-0abd-40b0-9fd8-923fe4a3201a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.3239964246749878,\n",
       "  0.6962459087371826,\n",
       "  0.47621896862983704,\n",
       "  0.3189353346824646,\n",
       "  0.20820467174053192,\n",
       "  0.1368134766817093],\n",
       " 'accuracy': [0.5922222137451172,\n",
       "  0.7752777934074402,\n",
       "  0.8394444584846497,\n",
       "  0.8930555582046509,\n",
       "  0.9375,\n",
       "  0.9641666412353516],\n",
       " 'val_loss': [0.9726228713989258,\n",
       "  0.9693282842636108,\n",
       "  0.9378651976585388,\n",
       "  0.9523977041244507,\n",
       "  1.0433592796325684,\n",
       "  1.1192824840545654],\n",
       " 'val_accuracy': [0.6877777576446533,\n",
       "  0.695555567741394,\n",
       "  0.7222222089767456,\n",
       "  0.7244444489479065,\n",
       "  0.7166666388511658,\n",
       "  0.7400000095367432]}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acbcc7c-bbbe-4057-9174-cc42fe4a2e49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf6720e-824b-4ed8-9e51-ef348c06f4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad393f7-782c-4c12-b788-28304841286e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
