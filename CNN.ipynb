{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85922cd2-5ed4-44b9-9f3a-fde0eff217ea",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b89cc8e-2217-441a-8aa0-4d35ad117795",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "import tensorflow_datasets as tfds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae96513-428f-4655-9504-d42f045b63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "datapath = 'clothing-dataset/'\n",
    "data = []\n",
    "with open(os.path.join(datapath, 'images.csv')) as csv_file:\n",
    "    reader = csv.reader(csv_file, delimiter=',')\n",
    "    for line in reader:\n",
    "        data.append(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c2fcdf-b963-4bdc-8ad5-ea75ccbe2e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584fafb3-d228-4f04-bbe1-c10b8f9b2411",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data[0]\n",
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109a017d-2769-4f46-8870-4f03fb09f552",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(labels)\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1f6f5a-1295-4aa0-b700-195d18f916b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = len(data)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1d49f8-1f00-4138-8c1e-1cfc3289ce72",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = dict()\n",
    "for row in data:\n",
    "    if row[2] not in label_counts:\n",
    "        label_counts[row[2]] = 0\n",
    "    label_counts[row[2]] += 1\n",
    "    \n",
    "valid_labels = dict()\n",
    "for k, v in label_counts.items():\n",
    "    if v >= 100 and k not in ['Not sure', 'Others', 'Skip']:\n",
    "        valid_labels[k] = v\n",
    "\n",
    "filename_to_label = dict()\n",
    "cleaned_data = []\n",
    "valid_filenames = []\n",
    "for i in range(len(data)):\n",
    "    if data[i][2] in valid_labels.keys():\n",
    "        cleaned_data.append(data[i])\n",
    "        valid_filenames.append(data[i][0])\n",
    "        filename_to_label[data[i][0]] = data[i][2]\n",
    "\n",
    "\n",
    "\n",
    "num_examples = len(cleaned_data)\n",
    "print(num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e4a5d-811c-43f2-bf02-8a253c58ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "imagespath = 'clothing-dataset/images'\n",
    "images = [f for f in os.listdir(imagespath) if os.path.isfile(os.path.join(imagespath, f))]\n",
    "\n",
    "num_images_moved = 0\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(images)):\n",
    "    filename = images[i].split(\".\")[0]\n",
    "    if filename not in valid_filenames:\n",
    "        continue\n",
    "        \n",
    "    if num_images_moved < num_train + num_validate:\n",
    "        folder_name = 'train'\n",
    "    else:\n",
    "        folder_name = 'test'\n",
    "    \n",
    "    new_folder = os.path.join(imagespath, folder_name)\n",
    "    \n",
    "    classname = filename_to_label[filename]\n",
    "    finalfoldername = os.path.join(new_folder, classname)\n",
    "    \n",
    "    if not os.path.exists(finalfoldername):\n",
    "        os.makedirs(finalfoldername)\n",
    "    \n",
    "    old_image_path = os.path.join(imagespath, images[i])\n",
    "    new_image_path = os.path.join(finalfoldername, images[i])\n",
    "    shutil.move(old_image_path, new_image_path)\n",
    "    num_images_moved += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e60d542-006b-459c-b28b-c720b6229169",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train = 3600\n",
    "num_validate = 900\n",
    "num_test = 461\n",
    "epochs = 50\n",
    "batch_size = 16\n",
    "train_dir = os.path.join(os.getcwd(), os.path.join(imagespath, 'train'))\n",
    "img_height = 128\n",
    "img_width = 128\n",
    "input_shape = (128, 128, 3)\n",
    "num_classes = len(valid_labels)\n",
    "labels = [\"Blazer\", 'Dress', 'Hat', 'Hoodie', 'Longsleeve', 'Outwear', 'Pants', 'Polo', 'Shirt', 'Shoes', 'Shorts', 'Skirt', 'T-Shirt', 'Undershirt']\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a18531a-c23d-4bf8-b758-e3d3db07164a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "seed = random.randint(1, 10000)\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), os.path.join(imagespath, 'test'))\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_dir,\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c54a86-f998-41a9-b223-441019acf585",
   "metadata": {},
   "outputs": [],
   "source": [
    "lmao = train_ds.take(1)\n",
    "fig, ax = plt.subplots(3, 3)\n",
    "for x, y in tfds.as_numpy(lmao):\n",
    "    for i in range(batch_size):\n",
    "        if i < 9:\n",
    "            ax[i // 3, i % 3].imshow(x[i] / 255)\n",
    "            ax[i // 3, i % 3].set_title(labels[y[i]])\n",
    "            ax[i // 3, i % 3].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b0137-63fa-44de-a9cf-c6bdb1ce4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"), \n",
    "                                         tf.keras.layers.RandomRotation(0.2),])\n",
    "model = tf.keras.Sequential([\n",
    "    data_augmentation, \n",
    "  tf.keras.layers.Rescaling(1./255),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "  tf.keras.layers.MaxPooling2D(),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800daa2f-8f8f-42e5-8aeb-2eac309e8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75435d4-93ed-4601-96f7-55cfd2e96bbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a7e8b5-502b-4f44-a0b8-f52592dacfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = tf.keras.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "predictor.compile(\n",
    "  optimizer='adam',\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "  metrics=['accuracy'])\n",
    "accuracy = predictor.evaluate(test_ds)[1]\n",
    "print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857558c-585a-451a-9de5-2b84c55a3348",
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = tf.keras.Sequential()\n",
    "resnet50 = tf.keras.applications.ResNet50V2(include_top=False,\n",
    "                    input_shape=(img_height,img_width,3),\n",
    "                    pooling='avg',classes=num_classes,\n",
    "                    weights='imagenet')\n",
    "\n",
    "#transfer_model.add(data_augmentation)\n",
    "\n",
    "for layer in resnet50.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "transfer_model.add(resnet50)\n",
    "transfer_model.add(Flatten())\n",
    "transfer_model.add(tf.keras.layers.BatchNormalization())\n",
    "transfer_model.add(Dense(128, activation='relu'))\n",
    "transfer_model.add(Dropout(0.1))\n",
    "transfer_model.add(tf.keras.layers.BatchNormalization())\n",
    "transfer_model.add(Dense(num_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8ec11-f1bb-4d95-abdd-b7d29c454b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transfer_model.compile(optimizer='adam',loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\n",
    "history = transfer_model.fit(train_ds, validation_data=val_ds, epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3c654a-b234-48e0-b3d6-5a7c1d8d82dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "accuracy = transfer_model.evaluate(test_ds)[1]\n",
    "print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9060c81e-ccb8-424a-b234-edf823137d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8685b6-792c-45ce-8824-f7dc7e6cd8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be87d6-95f9-4fd4-892d-90fcfae8fcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 14\n",
    "input_shape = (128, 128, 3)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 265\n",
    "hidden_units = 512\n",
    "projection_units = 128\n",
    "num_epochs = 50\n",
    "dropout_rate = 0.5\n",
    "temperature = 0.05\n",
    "\n",
    "\n",
    "import random\n",
    "seed = random.randint(1, 10000)\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"training\",\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  train_dir,\n",
    "  validation_split=0.2,\n",
    "  subset=\"validation\",\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), os.path.join(imagespath, 'test'))\n",
    "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "  test_dir,\n",
    "  seed=seed,\n",
    "  image_size=(img_height, img_width),\n",
    "  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f38a108-d19d-4001-87d8-0cb243f3da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder():\n",
    "    transfer_model = keras.applications.ResNet50V2(\n",
    "        include_top=False, weights=None, input_shape=input_shape, pooling=\"avg\"\n",
    "    )\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    outputs = transfer_model(inputs)\n",
    "    final_encoder = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return final_encoder\n",
    "\n",
    "\n",
    "initial_encoder = encoder()\n",
    "initial_encoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e7f717-aa9a-4c60-ba6c-1c54ee0848f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(initial_encoder, trainable=True):\n",
    "\n",
    "    for layer in initial_encoder.layers:\n",
    "        layer.trainable = trainable\n",
    "\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    learned_features = initial_encoder(inputs)\n",
    "    learned_features = layers.Dropout(dropout_rate)(learned_features)\n",
    "    learned_features = layers.Dense(hidden_units, activation=\"relu\")(learned_features)\n",
    "    learned_features = layers.Dropout(dropout_rate)(learned_features)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(learned_features)\n",
    "\n",
    "    final_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    final_model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edcb795-bb78-4791-9a4b-0f44995662d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedContrastiveLoss(keras.losses.Loss):\n",
    "    def __init__(self, temperature, name=None):\n",
    "        super(SupervisedContrastiveLoss, self).__init__(name=name)\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __call__(self, labels, feature_vectors, sample_weight=None):\n",
    "        normalized_feature_vectors = tf.math.l2_normalize(feature_vectors, axis=1)\n",
    "        calculated_logits = tf.divide(tf.matmul(normalized_feature_vectors, tf.transpose(normalized_feature_vectors)), self.temperature)\n",
    "        return tfa.losses.npairs_loss(tf.squeeze(labels), calculated_logits)\n",
    "\n",
    "\n",
    "def projection(initial_encoder):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    learned_features = initial_encoder(inputs)\n",
    "    outputs = layers.Dense(projection_units, activation=\"relu\")(learned_features)\n",
    "    final_model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25c9db-0db4-4c44-a4b3-67f745f60585",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_encoder = encoder()\n",
    "\n",
    "encoder_projection = projection(initial_encoder)\n",
    "encoder_projection.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate),\n",
    "    loss=SupervisedContrastiveLoss(temperature),\n",
    ")\n",
    "\n",
    "encoder_projection.summary()\n",
    "\n",
    "history = encoder_projection.fit(\n",
    "    train_ds, batch_size=batch_size, epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e6b762-0b86-4029-9258-44b3e76ee2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = encoderprojection.evaluate(test_ds)[1]\n",
    "print(f\"Accuracy: {accuracy * 100}%\")\n",
    "\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
